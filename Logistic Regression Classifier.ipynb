{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from theano import function, shared, pp\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "Building the model...\n",
      "cost:\n",
      "Elemwise{neg,no_inplace} [@A] ''   \n",
      " |Elemwise{true_div,no_inplace} [@B] ''   \n",
      "   |Sum{acc_dtype=float64} [@C] ''   \n",
      "   | |AdvancedSubtensor [@D] ''   \n",
      "   |   |Elemwise{log,no_inplace} [@E] ''   \n",
      "   |   | |Softmax [@F] ''   \n",
      "   |   |   |Elemwise{add,no_inplace} [@G] ''   \n",
      "   |   |     |dot [@H] ''   \n",
      "   |   |     | |data_features [@I]\n",
      "   |   |     | |weights [@J]\n",
      "   |   |     |DimShuffle{x,0} [@K] ''   \n",
      "   |   |       |bias [@L]\n",
      "   |   |ARange [@M] ''   \n",
      "   |   | |TensorConstant{0} [@N]\n",
      "   |   | |Subtensor{int64} [@O] ''   \n",
      "   |   | | |Shape [@P] ''   \n",
      "   |   | | | |data_labels [@Q]\n",
      "   |   | | |Constant{0} [@R]\n",
      "   |   | |TensorConstant{1} [@S]\n",
      "   |   |data_labels [@Q]\n",
      "   |Subtensor{int64} [@T] ''   \n",
      "     |Elemwise{Cast{float64}} [@U] ''   \n",
      "     | |Shape [@V] ''   \n",
      "     |   |AdvancedSubtensor [@D] ''   \n",
      "     |Constant{0} [@W]\n",
      "train_model:\n",
      "Elemwise{TrueDiv}[(0, 0)] [@A] ''   15\n",
      " |Sum{acc_dtype=float64} [@B] ''   14\n",
      " | |CrossentropySoftmaxArgmax1HotWithBias.0 [@C] ''   13\n",
      " |   |Dot22 [@D] ''   10\n",
      " |   | |Subtensor{int64:int64:} [@E] ''   7\n",
      " |   | | |<TensorType(float64, matrix)> [@F]\n",
      " |   | | |ScalarFromTensor [@G] ''   4\n",
      " |   | | | |Elemwise{mul,no_inplace} [@H] ''   1\n",
      " |   | | |   |TensorConstant{600} [@I]\n",
      " |   | | |   |<TensorType(int64, scalar)> [@J]\n",
      " |   | | |ScalarFromTensor [@K] ''   3\n",
      " |   | |   |Elemwise{Composite{(i0 * (i1 + i2))}} [@L] ''   0\n",
      " |   | |     |TensorConstant{600} [@I]\n",
      " |   | |     |TensorConstant{1} [@M]\n",
      " |   | |     |<TensorType(int64, scalar)> [@J]\n",
      " |   | |weights [@N]\n",
      " |   |bias [@O]\n",
      " |   |Elemwise{Cast{int32}} [@P] ''   9\n",
      " |     |Subtensor{int64:int64:} [@Q] ''   6\n",
      " |       |<TensorType(float64, vector)> [@R]\n",
      " |       |ScalarFromTensor [@G] ''   4\n",
      " |       |ScalarFromTensor [@K] ''   3\n",
      " |Elemwise{Cast{float64}} [@S] ''   8\n",
      "   |Elemwise{Composite{(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1) - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i4, i1, i2), i1), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1)), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i4, i1, i2), i1), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1)))}}[(0, 0)] [@T] ''   5\n",
      "     |Elemwise{Composite{(i0 * (i1 + i2))}} [@L] ''   0\n",
      "     |TensorConstant{0} [@U]\n",
      "     |Shape_i{0} [@V] ''   2\n",
      "     | |<TensorType(float64, vector)> [@R]\n",
      "     |TensorConstant{-1} [@W]\n",
      "     |Elemwise{mul,no_inplace} [@H] ''   1\n",
      "Gemm{inplace} [@X] ''   20\n",
      " |weights [@N]\n",
      " |TensorConstant{-0.13} [@Y]\n",
      " |InplaceDimShuffle{1,0} [@Z] 'data_features.T'   11\n",
      " | |Subtensor{int64:int64:} [@E] ''   7\n",
      " |CrossentropySoftmax1HotWithBiasDx [@BA] ''   18\n",
      " | |Alloc [@BB] ''   17\n",
      " | | |Elemwise{Inv}[(0, 0)] [@BC] ''   16\n",
      " | | | |InplaceDimShuffle{x} [@BD] ''   12\n",
      " | | |   |Elemwise{Cast{float64}} [@S] ''   8\n",
      " | | |Elemwise{Composite{(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1) - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i4, i1, i2), i1), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1)), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i4, i1, i2), i1), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1)))}}[(0, 0)] [@T] ''   5\n",
      " | |CrossentropySoftmaxArgmax1HotWithBias.1 [@C] ''   13\n",
      " | |Elemwise{Cast{int32}} [@P] ''   9\n",
      " |TensorConstant{1.0} [@BE]\n",
      "Elemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)] [@BF] ''   21\n",
      " |bias [@O]\n",
      " |TensorConstant{(1,) of 0.13} [@BG]\n",
      " |Sum{axis=[0], acc_dtype=float64} [@BH] ''   19\n",
      "   |CrossentropySoftmax1HotWithBiasDx [@BA] ''   18\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, input, input_dimensions, output_dimensions):\n",
    "        self.weights = shared(\n",
    "            value=np.zeros(\n",
    "                (input_dimensions, output_dimensions),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='weights',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.bias = shared(\n",
    "            value=np.zeros(\n",
    "                (output_dimensions,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='bias',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.p_label_given_features = T.nnet.softmax(T.dot(input, self.weights) + self.bias)\n",
    "        self.label_prediction = T.argmax(self.p_label_given_features, axis=1)\n",
    "        self.params = [self.weights, self.bias]\n",
    "    \n",
    "    def negative_log_likelihood(self, labels):\n",
    "        # T.log(self.p_label_given_features) is a matrix where each row is an example\n",
    "        # and each column is a class, and each element is the log probability of that\n",
    "        # example being in that class.\n",
    "        # T.log(self.p_label_given_features)[T.arange(labels.shape[0]),labels)]\n",
    "        # is a vector where each element is the log probability of the correct label given\n",
    "        # the model.\n",
    "        return -T.mean(T.log(self.p_label_given_features)[T.arange(labels.shape[0]), labels])\n",
    "    \n",
    "    def errors(self, labels):\n",
    "        if labels.ndim != self.label_prediction.ndim:\n",
    "            raise TypeError(\"labels should have the same shape as self.label_prediction\",\n",
    "                            ('labels', labels.type, 'label_prediction', self.label_prediction.type))\n",
    "        if labels.dtype.startswith('int'):\n",
    "            return T.mean(T.neq(self.label_prediction, labels))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "# Copy and pasted. Not tryna deal with this.          \n",
    "def load_data(dataset):\n",
    "    ''' Loads the dataset\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path to the dataset (here MNIST)\n",
    "    '''\n",
    "\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "\n",
    "    # Download the MNIST dataset if it is not present\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the data directory.\n",
    "        new_path = os.path.join(\n",
    "            os.path.split(os.path.realpath('__file__'))[0],\n",
    "            \"data\",\n",
    "            dataset\n",
    "        )\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        import urllib\n",
    "        origin = (\n",
    "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        )\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "    print('... loading data')\n",
    "\n",
    "    # Load the dataset\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    #train_set, valid_set, test_set format: tuple(input, target)\n",
    "    #input is an numpy.ndarray of 2 dimensions (a matrix)\n",
    "    #witch row's correspond to an example. target is a\n",
    "    #numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
    "    #the number of rows in the input. It should give the target\n",
    "    #target to the example with the same index in the input.\n",
    "\n",
    "    def shared_dataset(data_xy, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(np.asarray(data_x,\n",
    "                                            dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(data_y,\n",
    "                                            dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        # When storing data on the GPU it has to be stored as floats\n",
    "        # therefore we will store the labels as ``floatX`` as well\n",
    "        # (``shared_y`` does exactly that). But during our computations\n",
    "        # we need them as ints (we use labels as index, and if they are\n",
    "        # floats it doesn't make sense) therefore instead of returning\n",
    "        # ``shared_y`` we will have to cast it to int. This little hack\n",
    "        # lets ous get around this issue\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval\n",
    "\n",
    "def nth_batch(data_set, batch_size, index):\n",
    "    return data_set[index * batch_size : (1 + index) * batch_size]\n",
    "\n",
    "def sgd_optimization_mnist(learning_rate=0.13, epochs=1000, dataset='mnist.pkl.gz', batch_size=600): \n",
    "    datasets = load_data(dataset)\n",
    "    \n",
    "    train_set_features, train_set_labels = datasets[0]\n",
    "    validation_set_features, validation_set_labels = datasets[1]\n",
    "    test_set_features, test_set_labels = datasets[2]\n",
    "    \n",
    "    train_batches_count = train_set_features.get_value(borrow=True).shape[0] // batch_size\n",
    "    validation_batches_count = validation_set_features.get_value(borrow=True).shape[0] // batch_size\n",
    "    train_batches_count = train_set_features.get_value(borrow=True).shape[0] // batch_size\n",
    "    \n",
    "    print(\"Building the model...\")\n",
    "    \n",
    "    index=T.lscalar()\n",
    "    \n",
    "    data_features = T.matrix('data_features')\n",
    "    data_labels = T.ivector('data_labels')\n",
    "\n",
    "    classifier = LogisticRegression(input=data_features, input_dimensions=28*28, output_dimensions=10)\n",
    "\n",
    "    cost = classifier.negative_log_likelihood(data_labels)\n",
    "    \n",
    "    test_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(data_labels),\n",
    "        givens={\n",
    "            data_features: nth_batch(test_set_features, batch_size, index),\n",
    "            data_labels: nth_batch(test_set_labels, batch_size, index)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    validate_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(data_labels),\n",
    "        givens={\n",
    "            data_features: nth_batch(validation_set_features, batch_size, index),\n",
    "            data_labels: nth_batch(validation_set_labels, batch_size, index)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    weights_gradient = T.grad(cost=cost, wrt=classifier.weights)\n",
    "    bias_gradient = T.grad(cost=cost, wrt=classifier.bias)\n",
    "\n",
    "    updates = [(classifier.weights, classifier.weights - learning_rate * weights_gradient),\n",
    "               (classifier.bias, classifier.bias - learning_rate * bias_gradient)]\n",
    "    \n",
    "    train_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            data_features: nth_batch(train_set_features, batch_size, index),\n",
    "            data_labels: nth_batch(train_set_labels, batch_size, index)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # print(\"cost:\")\n",
    "    # theano.printing.debugprint(cost)\n",
    "    # print(\"train_model:\")\n",
    "    # theano.printing.debugprint(train_model)\n",
    "\n",
    "    \n",
    "sgd_optimization_mnist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
