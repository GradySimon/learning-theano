{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from theano import function, shared, pp\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building the model...\n",
      "Training model...\n",
      "epoch 1, minibatch 83/83, validation error 12.458333 %\n",
      "     epoch 1, minibatch 83/83, test error of best model 12.375000 %\n",
      "epoch 2, minibatch 83/83, validation error 11.010417 %\n",
      "     epoch 2, minibatch 83/83, test error of best model 10.958333 %\n",
      "epoch 3, minibatch 83/83, validation error 10.312500 %\n",
      "     epoch 3, minibatch 83/83, test error of best model 10.312500 %\n",
      "epoch 4, minibatch 83/83, validation error 9.875000 %\n",
      "     epoch 4, minibatch 83/83, test error of best model 9.833333 %\n",
      "epoch 5, minibatch 83/83, validation error 9.562500 %\n",
      "     epoch 5, minibatch 83/83, test error of best model 9.479167 %\n",
      "epoch 6, minibatch 83/83, validation error 9.322917 %\n",
      "     epoch 6, minibatch 83/83, test error of best model 9.291667 %\n",
      "epoch 7, minibatch 83/83, validation error 9.187500 %\n",
      "     epoch 7, minibatch 83/83, test error of best model 9.000000 %\n",
      "epoch 8, minibatch 83/83, validation error 8.989583 %\n",
      "     epoch 8, minibatch 83/83, test error of best model 8.958333 %\n",
      "epoch 9, minibatch 83/83, validation error 8.937500 %\n",
      "     epoch 9, minibatch 83/83, test error of best model 8.812500 %\n",
      "epoch 10, minibatch 83/83, validation error 8.750000 %\n",
      "     epoch 10, minibatch 83/83, test error of best model 8.666667 %\n",
      "epoch 11, minibatch 83/83, validation error 8.666667 %\n",
      "     epoch 11, minibatch 83/83, test error of best model 8.520833 %\n",
      "epoch 12, minibatch 83/83, validation error 8.583333 %\n",
      "     epoch 12, minibatch 83/83, test error of best model 8.416667 %\n",
      "epoch 13, minibatch 83/83, validation error 8.489583 %\n",
      "     epoch 13, minibatch 83/83, test error of best model 8.291667 %\n",
      "epoch 14, minibatch 83/83, validation error 8.427083 %\n",
      "     epoch 14, minibatch 83/83, test error of best model 8.281250 %\n",
      "epoch 15, minibatch 83/83, validation error 8.354167 %\n",
      "     epoch 15, minibatch 83/83, test error of best model 8.270833 %\n",
      "epoch 16, minibatch 83/83, validation error 8.302083 %\n",
      "     epoch 16, minibatch 83/83, test error of best model 8.239583 %\n",
      "epoch 17, minibatch 83/83, validation error 8.250000 %\n",
      "     epoch 17, minibatch 83/83, test error of best model 8.177083 %\n",
      "epoch 18, minibatch 83/83, validation error 8.229167 %\n",
      "     epoch 18, minibatch 83/83, test error of best model 8.062500 %\n",
      "epoch 19, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 20, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 21, minibatch 83/83, validation error 8.208333 %\n",
      "     epoch 21, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 22, minibatch 83/83, validation error 8.187500 %\n",
      "     epoch 22, minibatch 83/83, test error of best model 7.927083 %\n",
      "epoch 23, minibatch 83/83, validation error 8.156250 %\n",
      "     epoch 23, minibatch 83/83, test error of best model 7.958333 %\n",
      "epoch 24, minibatch 83/83, validation error 8.114583 %\n",
      "     epoch 24, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 25, minibatch 83/83, validation error 8.093750 %\n",
      "     epoch 25, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 26, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 27, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 28, minibatch 83/83, validation error 8.052083 %\n",
      "     epoch 28, minibatch 83/83, test error of best model 7.843750 %\n",
      "epoch 29, minibatch 83/83, validation error 8.052083 %\n",
      "epoch 30, minibatch 83/83, validation error 8.031250 %\n",
      "     epoch 30, minibatch 83/83, test error of best model 7.843750 %\n",
      "epoch 31, minibatch 83/83, validation error 8.010417 %\n",
      "     epoch 31, minibatch 83/83, test error of best model 7.833333 %\n",
      "epoch 32, minibatch 83/83, validation error 7.979167 %\n",
      "     epoch 32, minibatch 83/83, test error of best model 7.812500 %\n",
      "epoch 33, minibatch 83/83, validation error 7.947917 %\n",
      "     epoch 33, minibatch 83/83, test error of best model 7.739583 %\n",
      "epoch 34, minibatch 83/83, validation error 7.875000 %\n",
      "     epoch 34, minibatch 83/83, test error of best model 7.729167 %\n",
      "epoch 35, minibatch 83/83, validation error 7.885417 %\n",
      "epoch 36, minibatch 83/83, validation error 7.843750 %\n",
      "     epoch 36, minibatch 83/83, test error of best model 7.697917 %\n",
      "epoch 37, minibatch 83/83, validation error 7.802083 %\n",
      "     epoch 37, minibatch 83/83, test error of best model 7.635417 %\n",
      "epoch 38, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 39, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 40, minibatch 83/83, validation error 7.822917 %\n",
      "epoch 41, minibatch 83/83, validation error 7.791667 %\n",
      "     epoch 41, minibatch 83/83, test error of best model 7.625000 %\n",
      "epoch 42, minibatch 83/83, validation error 7.770833 %\n",
      "     epoch 42, minibatch 83/83, test error of best model 7.614583 %\n",
      "epoch 43, minibatch 83/83, validation error 7.750000 %\n",
      "     epoch 43, minibatch 83/83, test error of best model 7.593750 %\n",
      "epoch 44, minibatch 83/83, validation error 7.739583 %\n",
      "     epoch 44, minibatch 83/83, test error of best model 7.593750 %\n",
      "epoch 45, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 46, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 47, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 48, minibatch 83/83, validation error 7.708333 %\n",
      "     epoch 48, minibatch 83/83, test error of best model 7.583333 %\n",
      "epoch 49, minibatch 83/83, validation error 7.677083 %\n",
      "     epoch 49, minibatch 83/83, test error of best model 7.572917 %\n",
      "epoch 50, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 51, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 52, minibatch 83/83, validation error 7.656250 %\n",
      "     epoch 52, minibatch 83/83, test error of best model 7.541667 %\n",
      "epoch 53, minibatch 83/83, validation error 7.656250 %\n",
      "epoch 54, minibatch 83/83, validation error 7.635417 %\n",
      "     epoch 54, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 55, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 56, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 57, minibatch 83/83, validation error 7.604167 %\n",
      "     epoch 57, minibatch 83/83, test error of best model 7.489583 %\n",
      "epoch 58, minibatch 83/83, validation error 7.583333 %\n",
      "     epoch 58, minibatch 83/83, test error of best model 7.458333 %\n",
      "epoch 59, minibatch 83/83, validation error 7.572917 %\n",
      "     epoch 59, minibatch 83/83, test error of best model 7.468750 %\n",
      "epoch 60, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 61, minibatch 83/83, validation error 7.583333 %\n",
      "epoch 62, minibatch 83/83, validation error 7.572917 %\n",
      "     epoch 62, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 63, minibatch 83/83, validation error 7.562500 %\n",
      "     epoch 63, minibatch 83/83, test error of best model 7.510417 %\n",
      "epoch 64, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 65, minibatch 83/83, validation error 7.562500 %\n",
      "epoch 66, minibatch 83/83, validation error 7.552083 %\n",
      "     epoch 66, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 67, minibatch 83/83, validation error 7.552083 %\n",
      "epoch 68, minibatch 83/83, validation error 7.531250 %\n",
      "     epoch 68, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 69, minibatch 83/83, validation error 7.531250 %\n",
      "epoch 70, minibatch 83/83, validation error 7.510417 %\n",
      "     epoch 70, minibatch 83/83, test error of best model 7.500000 %\n",
      "epoch 71, minibatch 83/83, validation error 7.520833 %\n",
      "epoch 72, minibatch 83/83, validation error 7.510417 %\n",
      "epoch 73, minibatch 83/83, validation error 7.500000 %\n",
      "     epoch 73, minibatch 83/83, test error of best model 7.489583 %\n",
      "Optimization complete with best validation score of 7.500000 %,with test performance 7.489583 %\n",
      "The code run for 74 epochs, with 2.642163 epochs/sec\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, input, input_dimensions, output_dimensions):\n",
    "        self.weights = shared(\n",
    "            value=np.zeros(\n",
    "                (input_dimensions, output_dimensions),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='weights',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.bias = shared(\n",
    "            value=np.zeros(\n",
    "                (output_dimensions,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='bias',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.p_label_given_features = T.nnet.softmax(T.dot(input, self.weights) + self.bias)\n",
    "        self.label_prediction = T.argmax(self.p_label_given_features, axis=1)\n",
    "        self.params = [self.weights, self.bias]\n",
    "    \n",
    "    def negative_log_likelihood(self, labels):\n",
    "        # T.log(self.p_label_given_features) is a matrix where each row is an example\n",
    "        # and each column is a class, and each element is the log probability of that\n",
    "        # example being in that class.\n",
    "        # T.log(self.p_label_given_features)[T.arange(labels.shape[0]),labels)]\n",
    "        # is a vector where each element is the log probability of the correct label given\n",
    "        # the model.\n",
    "        return -T.mean(T.log(self.p_label_given_features)[T.arange(labels.shape[0]), labels])\n",
    "    \n",
    "    def errors(self, labels):\n",
    "        if labels.ndim != self.label_prediction.ndim:\n",
    "            raise TypeError(\"labels should have the same shape as self.label_prediction\",\n",
    "                            ('labels', labels.type, 'label_prediction', self.label_prediction.type))\n",
    "        if labels.dtype.startswith('int'):\n",
    "            return T.mean(T.neq(self.label_prediction, labels))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "# Copy and pasted. Not tryna deal with this.          \n",
    "def load_data(dataset):\n",
    "    ''' Loads the dataset\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path to the dataset (here MNIST)\n",
    "    '''\n",
    "\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "\n",
    "    # Download the MNIST dataset if it is not present\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the data directory.\n",
    "        new_path = os.path.join(\n",
    "            os.path.split(os.path.realpath('__file__'))[0],\n",
    "            \"data\",\n",
    "            dataset\n",
    "        )\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        import urllib\n",
    "        origin = (\n",
    "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        )\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "    print('Loading data...')\n",
    "\n",
    "    # Load the dataset\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    #train_set, valid_set, test_set format: tuple(input, target)\n",
    "    #input is an numpy.ndarray of 2 dimensions (a matrix)\n",
    "    #witch row's correspond to an example. target is a\n",
    "    #numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
    "    #the number of rows in the input. It should give the target\n",
    "    #target to the example with the same index in the input.\n",
    "\n",
    "    def shared_dataset(data_xy, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(np.asarray(data_x,\n",
    "                                            dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(data_y,\n",
    "                                            dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        # When storing data on the GPU it has to be stored as floats\n",
    "        # therefore we will store the labels as ``floatX`` as well\n",
    "        # (``shared_y`` does exactly that). But during our computations\n",
    "        # we need them as ints (we use labels as index, and if they are\n",
    "        # floats it doesn't make sense) therefore instead of returning\n",
    "        # ``shared_y`` we will have to cast it to int. This little hack\n",
    "        # lets ous get around this issue\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval\n",
    "\n",
    "def nth_batch(data_set, batch_size, index):\n",
    "    return data_set[index * batch_size : (1 + index) * batch_size]\n",
    "\n",
    "def sgd_optimization_mnist(learning_rate=0.13, epochs=1000, dataset='mnist.pkl.gz', batch_size=600): \n",
    "    datasets = load_data(dataset)\n",
    "    \n",
    "    train_set_features, train_set_labels = datasets[0]\n",
    "    validation_set_features, validation_set_labels = datasets[1]\n",
    "    test_set_features, test_set_labels = datasets[2]\n",
    "    \n",
    "    train_batch_count = train_set_features.get_value(borrow=True).shape[0] // batch_size\n",
    "    validation_batch_count = validation_set_features.get_value(borrow=True).shape[0] // batch_size\n",
    "    test_batch_count = test_set_features.get_value(borrow=True).shape[0] // batch_size\n",
    "    \n",
    "    print(\"Building the model...\")\n",
    "    \n",
    "    index=T.lscalar()\n",
    "    \n",
    "    data_features = T.matrix('data_features')\n",
    "    data_labels = T.ivector('data_labels')\n",
    "\n",
    "    classifier = LogisticRegression(input=data_features, input_dimensions=28*28, output_dimensions=10)\n",
    "\n",
    "    cost = classifier.negative_log_likelihood(data_labels)\n",
    "    \n",
    "    test_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(data_labels),\n",
    "        givens={\n",
    "            data_features: nth_batch(test_set_features, batch_size, index),\n",
    "            data_labels: nth_batch(test_set_labels, batch_size, index)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    validate_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(data_labels),\n",
    "        givens={\n",
    "            data_features: nth_batch(validation_set_features, batch_size, index),\n",
    "            data_labels: nth_batch(validation_set_labels, batch_size, index)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    weights_gradient = T.grad(cost=cost, wrt=classifier.weights)\n",
    "    bias_gradient = T.grad(cost=cost, wrt=classifier.bias)\n",
    "\n",
    "    updates = [(classifier.weights, classifier.weights - learning_rate * weights_gradient),\n",
    "               (classifier.bias, classifier.bias - learning_rate * bias_gradient)]\n",
    "    \n",
    "    train_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            data_features: nth_batch(train_set_features, batch_size, index),\n",
    "            data_labels: nth_batch(train_set_labels, batch_size, index)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    \n",
    "    patience = 5000\n",
    "    patience_increase = 2\n",
    "    improvement_threshold = 0.995\n",
    "    validation_frequency = min(train_batch_count, patience / 2)\n",
    "    best_validation_loss = np.inf\n",
    "    test_score = 0\n",
    "    start_time = time.clock()\n",
    "    \n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    \n",
    "    while epoch < epochs and not done_looping:\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(train_batch_count):\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            iteration = (epoch - 1) * train_batch_count + minibatch_index\n",
    "            if (iteration + 1) % validation_frequency == 0:\n",
    "                validation_losses = [validate_model(i) for i in range(validation_batch_count)]\n",
    "                this_validation_loss = np.mean(validation_losses)\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        train_batch_count,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                        patience = max(patience, iteration * patience_increase)\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    test_losses = [test_model(i) for i in range(test_batch_count)]\n",
    "                    test_score = np.mean(test_losses)\n",
    "                    print(\n",
    "                        (\n",
    "                            '     epoch %i, minibatch %i/%i, test error of'\n",
    "                            ' best model %f %%'\n",
    "                        ) %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            train_batch_count,\n",
    "                            test_score * 100.\n",
    "                        )\n",
    "                    )\n",
    "            if patience <= iteration:\n",
    "                done_looping = True\n",
    "                break\n",
    "    \n",
    "    end_time = time.clock()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%,'\n",
    "            'with test performance %f %%'\n",
    "        )\n",
    "        % (best_validation_loss * 100., test_score * 100.)\n",
    "    )\n",
    "    print('The code run for %d epochs, with %f epochs/sec' % (\n",
    "        epoch, 1. * epoch / (end_time - start_time)))\n",
    "    \n",
    "sgd_optimization_mnist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
